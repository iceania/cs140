			+--------------------+
			|        CS 140      |
         	        | PROJECT 1: THREADS |
		        |   DESIGN DOCUMENT  |
		        +--------------------+

---- GROUP ----

Matthew Chum-Lum <mchunlum@stanford.edu>
Sam Oluwalana <soluwalana@stanford.edu>
John-Ashton Allen  <bglp05@stanford.edu>

---- PRELIMINARIES ----
>> We couldn't get our project to pass tests when we use the PINTOSOPTS="-j #"
>> It works consistently with PINTOSOPTS="-r"


      ALARM CLOCK
     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

//thread.c
/* A list of blocked threads which are waiting for a certain tick to transpire.
   This list is iterated on on every timer interrupt to determine which thread
   to next wake up */
static struct list sleep_list;

//thread.h
struct thread {
	int64_t wake_time;           /* time used by thread sleep this is a tick on
              			     	which this thread will wake up */
}

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.
First we get the current tick.  Then we invoke thead_sleep with that tick plus the amount of 
ticks passed in to timer sleep (i.e. the tick we want it to wake up on).
Upon entering thread sleep, we get the current thread that is running, set it's wake time, 
and then disable interrupts.  With interrupts disabled we can push the thread onto the sleep list
and block the thread.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?
Since we used a linked list structure rather than a priority queue ordered by wake time
(we did not implement this as we were told that not doing so would not affect our grade), 
the best that we can do is O(n) time, with n being the number of sleeping threads.  
No other operations are performed besides the scanning of the list and removing threads that 
are being woken up(removing them from the sleeping list and putting them on the ready list).

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?
timer_sleep is just a wrapper to thread_sleep.  In thread_sleep we disable interrups
before we modify the sleep list, the only global data manipulated inside of the function.
We considered race conditions due to different threads being executed in parallel on multiple
proccesors; however, when we considered the implementation of locks and semaphores we realized
that they would elicit the same race condition, and thus not prevent this race condition.  Thus
we must assume that through building our operating system disabling interrupts only allows one thread
to run at any given moment.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?
Because we disabled interrrupts inside of thread_sleep, if we are interrupted, it will 
only be outside of a "critical section".

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered? 
We considered implementing a global list of blocked threads but we decided to 
take the approach that was already taken in the implementation of the semaphores
where the threads that are sleeping are waiting on time. So they should be put in 
a global sleep list rather than a generic global "blocked" list.

If you have a global list of blocked threads you lose the ability to control 
theads waiting on locks and semaphores.

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

//thread.h
struct thread {
	/* Shared between thread.c and synch.c. */
	int tmp_priority;            /* priority used for priority donation, will be increased
	    			     	if this thread holds a lock that a higher thread needs */
	struct list held_locks;      /* A list of locks that this thread currently owns*/
	struct lock* lock_waited_on;   /* A pointer to the lock that this thread is currently waiting on. */
}


//sync.h
/* Lock. No longer layered over a semaphore
 * It has a holder, a boolean value telling
 * whether it is held or not, and a list of waiters,
 * The lock_priority is the priority of the highest thread
   that is waiting on this lock. */
struct lock {
    struct thread *holder;      /* Thread holding lock . If no one is
     	 	 	 	  holding the lock it is available*/
    struct list_elem elem;	/* Element in a held locks list */
    struct list waiters;	/* List of waiting threads */
    int lock_priority; 	        /* The priority which is max over
				 * all threads which are waiting on this
				 * Will be the priority that that thread
				 * Which is holding the lock must have */
};

//sync.c
/* One semaphore in a list. */
struct semaphore_elem {
    struct list_elem elem;              /* List element. */
    struct semaphore semaphore;         /* This semaphore. */

    /* The thread that waits on this semaphore to become
     * available. */
    struct thread *thread;
};



>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)
   Threads:
   T1 = {Priority = 1}
   T2 = {Priority = 10} owns L2
   T3 = {Priority = 20}

   Locks:
   L1 with LockPriority = 0
   L2 with LockPriority = 0

Time increases veritcally.
Arrows indicate dependicies

t0       T1 acquires L1
t1       L1 LockPriority = 0
      	         ^
	         |
	         ^
t2  o------------+---------o<--<--<---T2 tries to acquire L1
t3  | L1 LockPriority= 10  |		    ^
t4  | T1 tmpPriority = 10  |		    |
    o----------------------o    	    ^
 		     			    |
t5	      o----------------------o------+<--<--<-- T3 tries to acquire L2
t6	      |	L2 lockPriority = 20 |
t7	      |	T2 tmpPriority  = 20 |
t8	      |	L1 lockPriority = 20 |
t9	      |	T1 tmpPriority  = 20 |
	      o----------------------o








---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

From the list of threads that are waiting on the synchronization primitives,
we take the thread with the max tmp priority to unblock when the primitive
releases or signals. Because this is done with interrupts disabled we are sure
that the tmp priorities can't change so we are guaranteed that we will run the 
thread with the highest priority that is waiting on this primitive.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When a call to lock_acquire comes in the interrupts are disabled. With the interrupts 
disabled we check to see if the lock has a holder. If it has a holder we put ourselves
on the waiters list for the lock. We update our tmp priority which will update the lock 
priority. Then we block. If it doesn't we put the lock on the the current threads list 
of held locks. 

Donation is handled in the update_tmp_priority function. 
The current thread first updates its tmp priority to the max between its priority 
and the lock priority of all the locks that it is currently holding. If the thread
is also waiting on a lock, the lock priority is updated to the max between its current
lock priority and the thread' tmp priority. If the lock priority was changed then we also
update the tmp priority of the lock's holder and repeat recursively until the nested donation is 
finished.


>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

First interrupts are disabled, then if the list of threads that are waiting on the lock
is not empty, we take the thread with the highest tmp priority out of the lock's waiting
list and unblock that thread (That thread is now on the ready list). 
We then update the lock's priority to the maximum tmp priority of threads that are still waiting 
on the lock to ensure that the lock has the correct lock priority for the next acquirer of the 
lock (this is because the highest priority has just been removed so the lock priority will 
most likely fall to a lower priority). Then the thread sets the lock's holder to null, removes 
the lock from its held lock list, updates the current thread's tmp priority so that its donated priority 
is reverted appropriately(The highest priority thread waiting on this thread is no longer waiting so the thread's tmp priority
will likely also decrease so we need to change it). Then it yields the cpu if it is no longer the highest thread 
(occurring when the tmp priority decreases).


---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

The potential race condition is that while updating our tmp priority we take the 
max lock priority of all of the locks that we are holding. These can all change while 
we are updating our tmp priority. This gives us the potential to set our tmp priority to
an invalid state. RACE CONDITION! This can only be resolved disabling interrupts because 
we also need to update tmp priorities while we are acquiring locks and releasing locks.
Inside these operations you can not use locks (obviously) so the lowest  synchronization
primitive is to disable interrupts whenever you need to update the tmp priority. 

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Priority donations only occur when waiting on locks. So when we were designing the
system we thought the best way to decompose the problem is to give the lock a priority
and that priority would correspond to how quickly this lock needs to be released. We 
thought that this would be more efficient than working with donating the priority directly
to the thread because the inderection removes the necessity for the threads to interact directly
with each other. This was better than other design decisions because the other designs didn't work.


			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

//thread.h
struct thread {
       	fixed_point recent_cpu;      /* The recent amount of cpu this thread has used.*/
       	int nice ;     /* Nice value of this thread used for mlfqs operations*/

}

//thread.c
/*The value passed to recalculate priority
 * In order to prevent it from moving a thread
 * from one ready queue to another ready queue*/
#define NO_SWITCH ((void*)1)

/* Queues used by the multi level feedback
 * queue scheduler */
static struct list mlfqs_queue[PRI_MAX+1];

/*Variable to track the load avg of the system*/
static fixed_point load_avg;

//fixed.point.c
typedef int32_t fixed_point;


---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0
 4
 8
12
16
20
24
28
32
36

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?